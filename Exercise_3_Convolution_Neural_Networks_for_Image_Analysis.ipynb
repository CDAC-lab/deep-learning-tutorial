{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 3 - Convolution Neural Networks for Image Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFUsJZSU-wJF",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 3 - Convolution Neural Networks for Image Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XusAABHD-58D",
        "colab_type": "text"
      },
      "source": [
        "In this workshop we will usea CNN to classify images in the  [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. \n",
        "\n",
        "We will classify the fashion products in Fashion MNIST  into 10 classes. The dataset contains 70,000 grayscale images, with resolution of 28x28 pixels.   \n",
        "We will use 60,000 images to train the classification CNN and 10,000 images to evaluate the accuracy of the network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRZEWyM6_fb1",
        "colab_type": "text"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9P3pEB_j13",
        "colab_type": "text"
      },
      "source": [
        "Generally we have to download the dataset from a repository and upload to use in our application. However, Keras python library provides us access to directly load a limited number of benchmark datasets.   \n",
        "Thus, we can access and load the Fashion-MNIST dataset directly from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7iIFLbF-Acy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Tensorflow library\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HWedSm3DPXh",
        "colab_type": "text"
      },
      "source": [
        "The dataset is already setup as training and testing sets. (60K for training and 10K for testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6migTJqA_yhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlpHxpHbAL9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View the dataset shape\n",
        "print('Train data shape:', X_train.shape)\n",
        "print('Test data shape:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIPjfH5RAXne",
        "colab_type": "text"
      },
      "source": [
        "X_train contains 60,000 train images with a resolution of 28x28.  \n",
        "The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td> \n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGMrPcgJAUnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TjH6DZ7Bj_b",
        "colab_type": "text"
      },
      "source": [
        "## Explore and pre-process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVSDAnwnAlkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load visualization library\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cguV38FYDgSQ",
        "colab_type": "text"
      },
      "source": [
        "We will plot an image from the dataset to get a feel for the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jnlp88TBtTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_index = 1\n",
        "plt.figure()\n",
        "plt.imshow(X_train[image_index])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYGKlpKUCRcm",
        "colab_type": "text"
      },
      "source": [
        "Display the first 25 images from the training set and display the class name below each image.   \n",
        "It is important to verify that the data is in the correct format before building the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brodZC5qBvNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[y_train[i]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd-lPPNaCfDQ",
        "colab_type": "text"
      },
      "source": [
        "A main step of image processing is to scale the data. With this step, we will be able observe  the range of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPn5SFTjCeI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import numpy library to process matrices\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZk_duGmB3XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('min:', np.min(X_train[image_index]))\n",
        "print('max:', np.max(X_train[image_index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MURb0-JMDNjr",
        "colab_type": "text"
      },
      "source": [
        "It is evident that the data ranges from 0-255.   \n",
        "We scale these values to a range of 0 to 1 before feeding to the neural network model.  \n",
        "For this, we divide the values by 255.   \n",
        "It's important that the training set and the testing set are preprocessed in the same way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEnN_wVdDLaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2U0u1n-L1j3",
        "colab_type": "text"
      },
      "source": [
        "Prepare for CNN processing by reshaping the image into (# training/testing samples, height, width, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXdZsPpcMCDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "height, width = 28, 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrAKyhukL9Bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], height, width, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], height, width, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClwHrfPUCwQt",
        "colab_type": "text"
      },
      "source": [
        "## Enable GPU\n",
        "\n",
        "CPUs are designed for more general computing workloads.  GPUs in contrast are less flexible, however GPUs are designed for parallel  computations. Deep Neural Networks (DNN) are structured in a very uniform manner such that at each layer of the network thousands of identical artificial neurons perform the same computation.   Therefore the structure of a DNN fits quite well with the kinds of computation that a GPU can efficiently perform.\n",
        "\n",
        "To enable GPU in Google Colab, \n",
        "\n",
        "\n",
        "1. Click on Runtime menu on the toolbar\n",
        "2. Select change runtime type\n",
        "3. Select GPU for Hardware Acceleration option list.\n",
        "4. Select SAVE.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyLSG4IyDtbX",
        "colab_type": "text"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdCD6-1NDwsq",
        "colab_type": "text"
      },
      "source": [
        "In this workshop, we will develop 3 deep learning models with increasing complexity to evaluate the classification accuracy.  \n",
        "\n",
        "\n",
        "1.   3 Layer DNN\n",
        "2.   1 Layer CNN\n",
        "3.   3 Layer CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGno2vDBEvnp",
        "colab_type": "text"
      },
      "source": [
        "### 3-DNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7TX5AmwFIc8",
        "colab_type": "text"
      },
      "source": [
        "In this DNN model, we flatten the 28x28 image into a 784 input feature vector.  \n",
        "It should be noted that with flattening we will face the inherent problems discussed in the class (i.e., scale, rotation, location, high-demensionality)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud-2924qDfqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Keras libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoTnr8D9E7oQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dnn3_model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0_5zNC3FAwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dnn3_model.add(Flatten(input_shape=(width, height, 1)))  # Add Keras Flatten layer to conver (28x28) image -> 784 feature vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKztFDk1FvBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dnn3_model.add(Dense(128, activation='relu')) \n",
        "dnn3_model.add(Dense(64, activation='relu')) \n",
        "dnn3_model.add(Dense(10, activation='softmax'))  # In the final layer we use softmax activation to classify the input into 10 classes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYJr_sT2GRsk",
        "colab_type": "text"
      },
      "source": [
        "Softmax is an activation function that turns logits into probabilities that sum to one. E.g.,\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " \n",
        "![alt text](https://engmrk.com/wp-content/uploads/2018/05/Fig1-3.jpg)\n",
        "\n",
        "For a complete understanding of available activation functions, please refer the following resources:  \n",
        "\n",
        "\n",
        "1.   [ML Cheatsheat](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n",
        "2.   [Towards data science](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0gwey-RIE-r",
        "colab_type": "text"
      },
      "source": [
        "**Compiling the model**\n",
        "\n",
        "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:  \n",
        "\n",
        "* Loss function —This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction. [Further details](https://keras.io/losses/) \n",
        "* Optimizer —This is how the model is updated based on the data it sees and its loss function. [Further details.](https://keras.io/optimizers/)\n",
        "* Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified. [Further details.](https://keras.io/metrics/) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAzocclQGQWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dnn3_model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfXpA1jNEjvg",
        "colab_type": "text"
      },
      "source": [
        "Initially, we will train the model with 10 learning epochs. Based on the learning curve, visualized below, you could update this hyper-parameter to best fit the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYNDvwNrIWRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "validation_split = 0.1\n",
        "history = dnn3_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split = validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2huim_yJjO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the training curve\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeftI0WOJDcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize accuracy (mean squared error)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NhftIe4J1Ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = dnn3_model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAOA4U3yKvw8",
        "colab_type": "text"
      },
      "source": [
        "### 1-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ikjZvn-LLv4",
        "colab_type": "text"
      },
      "source": [
        "As discussed in the lecture, CNN is specifically designed to process two-dimensional data spaces (focusing on image data).  \n",
        "First, we will design a single layer CNN. This will include one convolutional layer, one max pooling layer and two fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxLaLrbeKxWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import required libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mim4gnhxMmzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set input shape\n",
        "input_shape = (width, height, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpdMjo2nZGfU",
        "colab_type": "text"
      },
      "source": [
        "Construct the CNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KIAwoYwLdmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn1_model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62awqnJ5ZKHE",
        "colab_type": "text"
      },
      "source": [
        "Here we will use 3x3 filters (or kernals) to learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxUzzQMtLj5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn1_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))   # We will use 32 of 3x3 kernals to learn\n",
        "cnn1_model.add(MaxPooling2D(pool_size=(2, 2)))   # We use 2x2 max pooling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtQM_8QDZRRM",
        "colab_type": "text"
      },
      "source": [
        "Flattent the previous output before sending to fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKyeW1uZMz7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn1_model.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wgKQuLgZhq7",
        "colab_type": "text"
      },
      "source": [
        "We will use one fully connected (FC) layer with 128 nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgert5CQM-BE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn1_model.add(Dense(64, activation='relu'))\n",
        "cnn1_model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj7fxj6VNA9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "cnn1_model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMy-8xdfZo2U",
        "colab_type": "text"
      },
      "source": [
        "As before, initially we will train the model for 10 epochs to evaluate the performance.  \n",
        "Then, based on the accuracy/learning curve we will refine the hyper-parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RkPsb3rNIxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "validation_split = 0.1\n",
        "history_1 = cnn1_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split = validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKxLPmfENX5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn1_score = cnn1_model.evaluate(X_test, y_test)\n",
        "print('Test loss:', cnn1_score[0])\n",
        "print('Test accuracy:', cnn1_score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj0trFTsNljB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the training curve\n",
        "plt.plot(history_1.history['loss'])\n",
        "plt.plot(history_1.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR-gHsmLnzZH",
        "colab_type": "text"
      },
      "source": [
        "By analyzing the learning curve, you should be able to identify whether training the model longer would provide better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KfgXh5cgj7NH"
      },
      "source": [
        "### 3-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HXKbUov0j7NM"
      },
      "source": [
        "In this step, we will design a complex three layer CNN intending an improvement in the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tubeQl30j7NQ",
        "colab": {}
      },
      "source": [
        "# Import required libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YKRi5t1Lj7Nb",
        "colab": {}
      },
      "source": [
        "# Set input shape\n",
        "input_shape = (width, height, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uk433vWAj7Ng",
        "colab": {}
      },
      "source": [
        "cnn3_model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "msnxlKZoj7Nk",
        "colab": {}
      },
      "source": [
        "# CNN Layer 1\n",
        "cnn3_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "cnn3_model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# CNN Layer 2\n",
        "cnn3_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "cnn3_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# CNN Layer 3\n",
        "cnn3_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vTvTcnfOj7No",
        "colab": {}
      },
      "source": [
        "cnn3_model.add(Flatten())  # Flatten the previous output before sending to fully connected layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iLdtzybmj7Nr",
        "colab": {}
      },
      "source": [
        "cnn3_model.add(Dense(128, activation='relu'))\n",
        "cnn3_model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zcuHfQn5j7Nv",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "cnn3_model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ROuuFRIvj7N0",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "validation_split = 0.1\n",
        "history_3 = cnn3_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split = validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_C_rWaxUj7N5",
        "colab": {}
      },
      "source": [
        "cnn3_score = cnn3_model.evaluate(X_test, y_test)\n",
        "print('Test loss:', cnn3_score[0])\n",
        "print('Test accuracy:', cnn3_score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUeiwKS7oEez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the training curve\n",
        "plt.plot(history_3.history['loss'])\n",
        "plt.plot(history_3.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvzA59d2o9CT",
        "colab_type": "text"
      },
      "source": [
        "Based on the leanring curve, it seems after first epoch the model starts to overfit.  \n",
        "In such scenarios, **dropout** is used to mitigate the effect of overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRUZXaROpRza",
        "colab_type": "text"
      },
      "source": [
        "### 3-CNN with Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH-eSJlMpVZZ",
        "colab_type": "text"
      },
      "source": [
        "Dropout aims to solve the significant challenge of overfitting. It is one of the biggest advancements in deep learning proposed in recent years.\n",
        "\n",
        "The idea is very simple though, it is to randomly drop units in a deep neural network.\n",
        "\n",
        "Learning the relationship between the inputs and the outputs of a dataset is a very complicated procedure. If you have a very small dataset, the relationship maybe a result of noise in the input sample.\n",
        "\n",
        "Dropout refers to randomly and temporary removing a unit, either in a hidden or a visible layer, and all of its incoming and outgoing connections.\n",
        "\n",
        "Further reading on dropout: \n",
        "\n",
        "\n",
        "*   [Machine Learning Mastery](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)\n",
        "*   [Marko Jerkic](https://markojerkic.com/what-is-dropout-deep-learning/)\n",
        "\n",
        "![alt text](https://i1.wp.com/cdn-images-1.medium.com/max/800/1*iWQzxhVlvadk6VAJjsgXgg.png?resize=800%2C398&ssl=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901wJ7o3p9qF",
        "colab_type": "text"
      },
      "source": [
        "Let's now attempt the same 3-CNN model with **dropout**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZcyXqU8qfqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dropout from keras library\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNxRpV75oqtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Updated model layers\n",
        "cnn3_dp_model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_VjK_zla_39",
        "colab_type": "text"
      },
      "source": [
        "Now in each layer, we will add a dropout. The dropout rate is a hyper-parameter we could select based on our model requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6IdzgRgBqOJD",
        "colab": {}
      },
      "source": [
        "# CNN Layer 1\n",
        "cnn3_dp_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "cnn3_dp_model.add(MaxPooling2D((2, 2)))\n",
        "cnn3_dp_model.add(Dropout(0.3))\n",
        "\n",
        "# CNN Layer 2\n",
        "cnn3_dp_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "cnn3_dp_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn3_dp_model.add(Dropout(0.3))\n",
        "\n",
        "# CNN Layer 3\n",
        "cnn3_dp_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "cnn3_dp_model.add(Dropout(0.3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H_ce3upTqOJH",
        "colab": {}
      },
      "source": [
        "cnn3_dp_model.add(Flatten())  # Flatten the previous output before sending to fully connected layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98D6ImkWqOJJ",
        "colab": {}
      },
      "source": [
        "cnn3_dp_model.add(Dense(64, activation='relu'))\n",
        "cnn3_dp_model.add(Dropout(0.3))\n",
        "cnn3_dp_model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iG4s4WqkqOJM",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "cnn3_dp_model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NUOt0HYGqOJQ",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "epochs = 15\n",
        "batch_size = 32\n",
        "validation_split = 0.1\n",
        "history_3_dp = cnn3_dp_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split = validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q7Vln5efqOJU",
        "colab": {}
      },
      "source": [
        "cnn3_dp_score = cnn3_dp_model.evaluate(X_test, y_test)\n",
        "print('Test loss:', cnn3_dp_score[0])\n",
        "print('Test accuracy:', cnn3_dp_score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p9aGtsRuqOJY",
        "colab": {}
      },
      "source": [
        "# Visualize the training curve\n",
        "plt.plot(history_3_dp.history['loss'])\n",
        "plt.plot(history_3_dp.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Jyfotz9B_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history_3_dp.history['accuracy'])\n",
        "plt.plot(history_3_dp.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhPwcElqt1Df",
        "colab_type": "text"
      },
      "source": [
        "We can now see the overfitting problem has been solved by adding dropout.  \n",
        "In order to improve accuracy, we could train the model for longer - more epochs (Hint: Try 20, 30, 40, 50, etc.)"
      ]
    }
  ]
}