{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 1 v2- Deep Neural Networks for Standard Classification Problems - Price Prediction",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtGTpd6g0r_r",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 1 - Deep Neural Networks for Standard Classification Problems - Price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJr9iTt600FD",
        "colab_type": "text"
      },
      "source": [
        "With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this dataset challenges you to predict the final price of each home. The Kaggle challenge can be found [Here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El2XmruGPb1A",
        "colab_type": "text"
      },
      "source": [
        "Load required libraries for modeling and data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9QlV0BC_9PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Az32Yl455cZ",
        "colab_type": "text"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "928acFRDBVev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVGQ7TRGA4Sm",
        "colab_type": "text"
      },
      "source": [
        "Authenticate and create the PyDrive client."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6Kbe6Zu_kD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ArRh-xt_3FF",
        "colab_type": "text"
      },
      "source": [
        "When prompted, click on the link to get authentication to allow Google to access your Drive. You should see a screen with “Google Cloud SDK wants to access your Google Account” at the top. After you allow permission, copy the given verification code and paste it in the box in Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-MgKXCw_kGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_file_id = '1RBxydSbuwpMCVaJ2t6cHyNFYMGYvaD8P'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySvIiqpN_peM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':dataset_file_id}) \n",
        "downloaded.GetContentFile('kaggle_housing_cleaned.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AimvnJbPP1Lq",
        "colab_type": "text"
      },
      "source": [
        "Load the dataset file to a dataframe using Pandas library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyyMlpxz9H7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('kaggle_housing_cleaned.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1nG8LUP9oht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqcXsifZQCy3",
        "colab_type": "text"
      },
      "source": [
        "Data description can be found from [this link](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPy3xiAjQe8F",
        "colab_type": "text"
      },
      "source": [
        "We will use SalePrice variable as the predictor and store the predictor array using variable *Y*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXsJPvZW-V9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = df[['SalePrice']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6jEfGrBXuzPB"
      },
      "source": [
        "## Exploratory Analytics and pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BKs3exqVuzPA"
      },
      "source": [
        "First we will remove ID column and target column from the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CwFn5zv5uzO9",
        "colab": {}
      },
      "source": [
        "df.drop(['Id', 'SalePrice'], inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yc9Si6hmuzO6",
        "colab": {}
      },
      "source": [
        "print('Dataset size: Rows - {}, Columns - {}'.format(df.shape[0], df.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC4Fl_-X_Ntz",
        "colab_type": "text"
      },
      "source": [
        "Note that for the experiment, we will only use the continuous variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SPUxApY8uzO3",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MLZTsSkSuzOa"
      },
      "source": [
        "For the experiment, we will only use the continuous variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4-SM0l6veZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_numerical = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hbL69PFGuzOZ"
      },
      "source": [
        "### Standardization\n",
        "\n",
        "First we will have a look on the data distribution using a box plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDGSttVPyN-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_numerical.boxplot(rot=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV25U7ClyzcK",
        "colab_type": "text"
      },
      "source": [
        "We will normalize the continous variables using Min Max Normalization technique.  \n",
        "Seperate normalizing objects are used for features and target variable(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZYEgVyBiuzOS",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "target_scaler = MinMaxScaler(feature_range=(0, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A0NYNfp6uzOM",
        "colab": {}
      },
      "source": [
        "df_numerical = feature_scaler.fit_transform(df_numerical)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6GkmZW8fuzOD",
        "colab": {}
      },
      "source": [
        "Y_scaled = target_scaler.fit_transform(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTXsIezoKwGf",
        "colab_type": "text"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXa8K6n2av5R",
        "colab_type": "text"
      },
      "source": [
        "In this workshop, we use Keras API to develop the deep neural network (DNN), on top of Tensorflow framework.  \n",
        "Further details on the Keras API and how to customize models can be learnt from [the official Keras Guide](https://keras.io/getting-started/functional-api-guide/).  \n",
        "  \n",
        "The DNN model we will use is shown below.\n",
        "\n",
        "![alt text](https://i.imgur.com/4cyoPiL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_gfW7TNSK48",
        "colab_type": "text"
      },
      "source": [
        "Import Keras library with Tensorflow and sklearn for model development"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwniw_omKfyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMk7A2GWST1y",
        "colab_type": "text"
      },
      "source": [
        "Split the dataset (use 70/30 for train/test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnqmS8VSfLEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_numerical, Y_scaled, test_size=0.3, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWhr4ov0fNg2",
        "colab_type": "text"
      },
      "source": [
        "First we will define a sequential model, which is the placeholder for our deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97GnODyDasMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMJk9FmkfUKO",
        "colab_type": "text"
      },
      "source": [
        "Next we will setup the first layer of our deep neural network (DNN). Here we initialize the input dimentions.  \n",
        "Note that we will use 36 hidden nodes for each layer of our DNN. You may vary them in own experimental setup, in order to improve the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS6cQ1knbHw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(36, kernel_initializer='normal', input_dim = X_train.shape[1], activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwGhqZLKftcf",
        "colab_type": "text"
      },
      "source": [
        "After initiaing the first layer, we will define 2nd, 3rd and 4th layers similarly.  \n",
        "However, we do not need to define the input dimensions in proceeding layers, as it will automatically detected from first layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-9AI9M9bb0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(24, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(12, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(8, kernel_initializer='normal',activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M0LQGsLgCBl",
        "colab_type": "text"
      },
      "source": [
        "Next we will define the output layer. As our output is a prediction of housing price, we will use a single linear activated output node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLXTxGZMbmZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST8CM7IfggRd",
        "colab_type": "text"
      },
      "source": [
        "Now we have completely defined the DNN model.  \n",
        "Next step is to compile the DNN with [loss function](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23), [optimization](https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3) function and metrics.  \n",
        "In our experiment, we will use Mean Absolute Error loss as the loss function, and ADAM optimizer as the optimization function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruzsrlR4bpWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the DNN\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHR1tj81buy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the model summary\n",
        "NN_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4UzS0whU5H",
        "colab_type": "text"
      },
      "source": [
        "Now we will plot the model in a diagram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbRA3W2ebxWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the model\n",
        "from keras.utils import plot_model\n",
        "plot_model(NN_model, to_file='model.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CK8geh9hYvG",
        "colab_type": "text"
      },
      "source": [
        "Once the plotting is completed, you can go to Files tab and double click on the model.png file to visualize the model diagram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pd9QZ9shMOd",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za6nIeaVh4Hl",
        "colab_type": "text"
      },
      "source": [
        "Now, we will work on training the model.  \n",
        "First we need to define 3 parameters,  \n",
        "\n",
        "\n",
        "1.   Number of training epochs\n",
        "2.   [The batch size](https://radiopaedia.org/articles/batch-size-machine-learning). i.e., how many training samples are used to iterate over once.\n",
        "3.   Validation split (what percentage of data to keep as validation data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSG5uY4Wb6s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100             # Number of training epochs\n",
        "batch_size = 32          # Number of data points to be used to train as a batch. Use to improve the model training time.\n",
        "validation_split = 0.3   # Validation dataset size (percentage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q88GNThi0hp",
        "colab_type": "text"
      },
      "source": [
        "By calling model.fit(), you can initiate the training of the DNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o4H-zjycy9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "history = NN_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split = validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh6BeDbti7dm",
        "colab_type": "text"
      },
      "source": [
        "Plot the learning curve, oppose to traning and validation errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9BocNJddB5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVvwREyHhSaU",
        "colab_type": "text"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCxSVIB_jjaQ",
        "colab_type": "text"
      },
      "source": [
        "Now we will test the trained DNN model with respect to the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYkasY3Vg8eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicted output for test dataset\n",
        "yhat_test = NN_model.predict(X_test)\n",
        "yhat_train = NN_model.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir21wRLLCZXF",
        "colab_type": "text"
      },
      "source": [
        "Recall how we standardized the data using scaler transform library in scikit-learn. Now, we will inverse transform the predictions back to its original range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6UOHEgOuKF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inverse transform test dataset\n",
        "inv_yhat = target_scaler.inverse_transform(yhat_test)\n",
        "inv_y_test = target_scaler.inverse_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD3LxVmouKIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inverse transform train dataset\n",
        "yhat_train = target_scaler.inverse_transform(yhat_train)\n",
        "inv_y_train = target_scaler.inverse_transform(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EARJip0EjzA6",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the root means squared error (RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2WQWg7_q__4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO-Q3LwBuKN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error_train = np.sqrt(mean_squared_error(inv_y_train, yhat_train))\n",
        "error_test = np.sqrt(mean_squared_error(inv_y_test, inv_yhat))\n",
        "print('Train RMSE: ', error_train)\n",
        "print('Test RMSE: ', error_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQyMAQI4C4Qv",
        "colab_type": "text"
      },
      "source": [
        "Relate the train and test error with bias/variance.  \n",
        "*  What is the problem we have here?\n",
        "*  What options we can take to improve the accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBGIMHyyuKLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(5, 10))\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
        "plt.subplots_adjust(hspace=3, wspace=1)\n",
        "ax[0].scatter(inv_y_test, inv_yhat, c='g')\n",
        "ax[0].set(title='Test data', xlabel='Actual Sale Price', ylabel='Predicted Sale Price')\n",
        "ax[1].scatter(inv_y_train, yhat_train, c='b')\n",
        "ax[1].set(title='Train data', xlabel='Actual Sale Price', ylabel='Predicted Sale Price')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgvSKt2DLfK",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqeHU4r2D501",
        "colab_type": "text"
      },
      "source": [
        "Adjusting/finding good values for hyperparameters is a slow process. You have to wait for the whole training process to complete, evaluate the results and adjust the value(s).  \n",
        "\n",
        "In general, hyperparameter tuning can give you 5-15% accuracy boost on the test data.  \n",
        "\n",
        "There are number of libraries to ease the process of hyperparameter tuning. Hyperas library [Link to library](https://github.com/maxpumperla/hyperas).  \n",
        "\n",
        "Credits to Nils Schlüter for the guide on [running hyperas with Google Colab](https://towardsdatascience.com/keras-hyperparameter-tuning-in-google-colab-using-hyperas-624fa4bbf673)."
      ]
    }
  ]
}